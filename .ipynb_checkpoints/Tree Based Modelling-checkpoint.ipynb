{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* High accuracy, stability and ease of interpretation\n",
    "* Allow for mapping of non-linear relationships\n",
    "* Adaptable at solving both classification and regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY WORDS: decision trees, random forest, gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithm (with predefined target variable), mostly used in classification problems. It works for both categorical and continuous input and output variables. The population or sample is split into sub-populations based on the most significant splitter/differentiatior in inut variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROOT NODE: represents the entire sample, which gets further divided into two or more subsets.\n",
    "* SPLITTING: process of dividing a node into two or more subnotes.\n",
    "* DECISION NODE: when a subnode splits into further sub-nodes\n",
    "* TERMINAL NODE / LEAF: a node which does not split any further\n",
    "* PRUNING: removal of subnodes (opposite of splitting)\n",
    "* BRANCH / SUBTREE: subsection of entire treee\n",
    "* PARENT AND CHILD NODES: node that is divided and nodes they get divided into to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](dt_terminology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easy to understand: even for people from non technical background; it does not need any statistical knowledge to read and interpret them. Intuitive graphical representation, easy to relate to initial hypothesis.\n",
    "* Useful in data expliration: decision trees are one of the fastest way to identify most significant variables and the relation among them. This method can help to create new features with more power to predict target variable. It can also help to identify most significant variables in the data exploration stage\n",
    "* Less data cleaning required: not influenced by outliers and missing values to a fair degree\n",
    "* No data type constraints: can handle both numerical and categorical variables\n",
    "* Non-parametric method: decision trees have no assumptions about the space distribution and the classifier structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Disavantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overfitting: to solve this problem set constraints on model parameteres and apply pruning\n",
    "* Not fit for continuous variables: while working with continuous variables, decision trees loose information when they categorize them into different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION TREE vs CLASSIFICATION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression trees are used when dependent variable is continuous. Classification trees are used when dependent variable is categorical.\n",
    "* In case of regression tree, the value obtained by terminal nodes in the training data is the mean response of observation falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mean value.\n",
    "* In case of classification tree, the value (class) obtained by terminal node in the training data is the mode of observations falling in that region. Thus, if an unseen data observation falls in that region, we’ll make its prediction with mode value.\n",
    "* Both the trees divide the predictor space (independent variables) into distinct and non-overlapping regions. For the sake of simplicity, you can think of these regions as high dimensional boxes or boxes.\n",
    "* Both the trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.\n",
    "* This splitting process is continued until a user defined stopping criteria is reached. For example: we can tell the the algorithm to stop once the number of observations per node becomes less than 50.\n",
    "* In both the cases, the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to overfit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting. We’ll learn more about it in following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](types_of_decision_trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING OF TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different\n",
    "for classification and regression trees. \n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on type of target variables. There are four most commonly used algorithms in decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) GINI index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Chi Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Reduction in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY PARAMETERS OF TREE MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODELS vs LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 98.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "# for classification, here you can change the algorithm as gini or entropy (information gain) \n",
    "# by default it is gini  \n",
    "model = tree.DecisionTreeClassifier(criterion='gini', splitter='best', random_state=1) \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n",
    "#Predict Output\n",
    "y_pred = model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "for i in range(1, len(y_test)):\n",
    "    res = 'correct' if y_test[i-1]==y_pred[i-1] else 'wrong'\n",
    "    # print(y_test[i-1], y_pred[i-1], res)\n",
    "\n",
    "print('Classification score: {}%'.format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split between train and test is fixed, by defining the random state.  if you use random_state=some_number, then you can guarantee that the output of Run 1 will be equal to the output of Run 2, i.e. your split will be always the same. It doesn't matter what the actual random_state number is 42, 0, 21, ... The important thing is that everytime you use 42, you will always get the same output the first time you make the split. This is useful if you want reproducible results, for example in the documentation, so that everybody can consistently see the same numbers when they run the examples. In practice I would say, you should set the random_state to some fixed number while you test stuff, but then remove it in production if you really need a random (and not a fixed) split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q: If the variance is not coming from the training set, then why is the result not always the same? Is there variability inroduced in the decision tree classifier? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The DecisionTreeClassifier works by repeatedly splitting the training data, based on the value of some feature. The Scikit-learn implementation lets you choose between a few splitting algorithms by providing a value to the splitter keyword argument.\n",
    "\n",
    "\"best\" randomly chooses a feature and finds the 'best' possible split for it, according to some criterion (which you can also choose; see the methods signature and the criterion argument). It looks like the code does this N_feature times, so it's actually quite like a bootstrap.\n",
    "\n",
    "\"random\" chooses the feature to consider at random, as above. However, it also then tests randomly-generated thresholds on that feature (random, subject to the constraint that it's between its minimum and maximum values). This may help avoid 'quantization' errors on the tree where the threshold is strongly influenced by the exact values in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE METHODS IN TREE BASED MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM vs XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
